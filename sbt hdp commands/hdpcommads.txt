executing mapreduce job  on a cluster:-------
hadoop jar JARNAME PACKAGE.CLASSNAME(CLASS CONTAING MAIN MENTHOD WITH JOB,CONFIGURATION) INPUTFILENAME(should exist in HDFS) OUTPUTFILENAME(shouldnot exist in hdfs,after job is completed it will be created)
hadoop jar /home/mydatasciencepractice1017/jars/customwordcount.jar customwordcount.CustomWordCount newtext.txt  count
hadoop fs -copyToLocal /user/ganesh4ja2452/chicagocrimedatacount/part-r-00000 /home/ganesh4ja2452


executing spark jobs on cluster------------------------


--------------------classname------------------------------------------------------jar location-------------------
spark-submit --class ScalaWordCount  --master yarn --deploy-mode cluster  /home/ganesh4ja2452/untitled_2.11-0.1.jar